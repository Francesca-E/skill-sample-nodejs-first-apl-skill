
:link-cakewalk: https://developer.amazon.com/en-US/alexa/alexa-skills-kit/courses/cake-walk[Cake Walk course]
:imagesdir: ../modules/images
:toc:

= Welcome

{blank}

Welcome to the Alexa Presentation Language (APL) extension to the {link-cakewalk}. After taking this course, you will have the information needed to create rich multimodal experiences for Alexa. The course will teach you the following:

* link:module1.html[Module 1]: Why build multimodal skills and the basics of an APL skill
* link:module2.html[Module 2]: How to use the authoring tool to create APL visuals
* link:module3.html[Module 3]: Modify your backend code to add a LaunchRequest visual
* link:module4.html[Module 4]: Make more visuals, scalably by sharing your custom APL code
* link:module5.html[Module 5]: Adding animations and controlling video with commands
* link:module6.html[Module 6]: Wrap-up and extra credit ideas

== What is Cake Walk?
Cake Walk is the introductory skill building course where new Alexa Developers build a simple Alexa skill which remembers their birthday. On repeated usage of the Cake Walk skill, you will either hear a Happy Birthday message from Alexa, or a countdown of days until it is your birthday. In this course, we are going to add visuals and animations to teach the fundamentals of building with APL. If you are new to skill building, we recommend you check out the {link-cakewalk}, but if you are already building Alexa skills or familiar with the concepts, then keep reading! Cake Walk starter code will be provided later if needed. 

== Why Multimodal Skills?
Multimodal simply means many modes. While Alexa is always voice first, adding visuals as a secondary mode can greatly enrich your customer's experience for Alexa-enabled devices with screens. There are tens of millions of multimodal Alexa devices, including Echo devices such as the Echo Spot and Echo Show, FireTV, Fire tablets, and thousands of third party devices. Adding another mode to your Alexa Skill can enhance the experience for your customers on these devices. 

=== Increase the Level of Detail of your Skill's Response
Multimodal Skills can simplify the voice responses from your Alexa Skill and provide more information through visuals. Voice is a fantastic interface for ease of use, speed and efficiency, but it is not always the best at presenting complex or a large amounts of information. Reducing the length of output speech if a screen is present and presenting information on the screen can help with verbose wording. For example, a weather skill that provides the daily forecast could just tell you the temperature range and precipitation, while on screen, it displays a breakdown by hour of each of these datapoints, humidity, wind speed, etc. All of the information is potentially important, but it would take far too long to even read out all of these stats, let alone an hour by hour breakdown! If the requesting device does not have a screen, more information such as humidity and wind speed can be read out to the customer. In both cases, you would want to allow them to ask for more details. This is especially important for when the customer is not looking at the screen, but using a multimodal device, anyways. Later in this course, we will cover the specifics of how to determine if the requesting Alexa device has a screen.

=== Ambient Experiences
Ambient experiences tangential to the main reason of opening the skill are an important way to train your customers in how to interact with your skill. Hints provide a place where you can nudge users to perform specific actions. This can be made specific to the interaction they are currently having. For instance, considering the weather skill above, you may want to provide a hint to the user to tell them that they can ask for a breakdown of the wind speed by hour if asking about the weather for the day. In addition to hints, you can display images related to the content you are talking about. This is particularly nice in longer running entertainment skills such as music skills where you can display the album art of a song playing, or even a photo gallery of the band at live shows. 

=== Complementary Branded Visuals
High quality visuals improves your brand image for your skill. Multimodal devices provide an opportunity to add visual branding to your experience. You can add your brand logo, as well as any information that might be relevant to the skill response. For instance, if you have a business skill, you might want to show a chart of sales when talking about a sales summary with your skill logo at the top. This complements the audio experience with a visual that is relevant to the voice response, in addition to your brand image. 

=== Rich Media Experiences
While you always need a voice experience, some skills are used primarily as a media experience. Multimodal interfaces give you the ability to provide videos and images. Consider a skill which displays user photos, videos, and photo metadata from a user's hosted drive account. This would do very little on a headless device, only having the functionality to read out the data about the content, but it would function fully and play the video or display images if used from a device with a screen. Multimodal devices give the opportunity to do this which does not exist otherwise. 

== Alexa Presentation Language
The https://developer.amazon.com/docs/alexa-presentation-language/understand-apl.html[Alexa Presentation Language], which we call APL for short, is designed for rendering visuals across the ever increasing category of Alexa-enabled devices with screens, allowing you to add graphics, images, slideshows, video, and animations to create the visual experience you want in a way that is consistent across your skill. 

=== APL Skill Flow

image:APLSkillFlowDiagram.png[]

Alexa skills all follow a diagram similar to the one above. Customers talk to their Alexa-enabled device (1). This goes off to Alexa Voice Services (AVS) to translate that voice into text, then text into an intention (2). Following that, the intent (with slots) goes to the correct backend (3) which formulates an appropriate speakOutput with optional additional directives. Directives are simply messages which tell an Alexa device to perform an action. This goes through AVS (4) to perform text to speech from the skill's speakOutput and renders visuals if the appropriate directive (RenderDocument) is sent. There are also events which can trigger requests to the skill backend in response to user actions. These are similar to the handlers created for an intent or launch request. 

=== APL Documents

An APL document is a holder for all of the data to define what happens in a RenderDocument directive. The visuals on screen are made up of APL components, and layouts. https://developer.amazon.com/docs/alexa-presentation-language/apl-component.html[Components] are the most basic building blocks of APL and represent small, self-contained UI elements which display on the https://en.wikipedia.org/wiki/Viewport[viewport]. https://developer.amazon.com/docs/alexa-presentation-language/apl-layout.html[Layouts] are used similarly to components, but are not primitive elements. Rather, they combine other layouts and primitive components to create a UI pattern. These can be self defined or pulled from other sources as an import. Every APL document has a "mainTemplate". This simply represents the start state of the APL screen to be rendered. Here is an example blank APL document with all of its properties: 

 {
     "type": "APL",
     "version": "1.1",
     "settings": {},
     "theme": "dark",
     "import": [],
     "resources": [],
     "styles": {},
     "onMount": [],
     "graphics": {},
     "commands": {},
     "layouts": {},
     "mainTemplate": {
         "parameters": [
             "payload"
         ],
         "items": []
     }
 }

Instead of defining all of the terms now, as you go through the course, you will learn many of the above parts of the APL document in more detail. 

link:module2.html[Next Module (2)]
