
:imagesdir: ../modules/images
:toc:

= Why Build Multimodal Alexa Skills?

{blank}

Multimodal is the addition of other forms of communication, such as visual aids, to the voice experience that Alexa already provides. While Alexa is always voice first, adding visuals as a secondary mode can greatly enrich your customer's experience for Alexa-enabled devices with screens. There are millions of multimodal Alexa devices, including Echo devices such as the Echo Spot and Echo Show, FireTV, Fire tablets, and thousands of devices from other manufacturers with Alexa built-in like the Lenovo Smart Tab devices and LG TVs. Adding another mode to your Alexa Skill can enhance the experience for your customers on these devices in the following ways.

== Increase the level of detail in your skill's response
Multimodal skills can provide more information through visual aids leading to a better customer experience. Voice is a fantastic way to interact with users because it's intuitive and efficient; however, it is not a great choice for presenting complex or large amounts of information at once. Having the ability to show visual aids can reduce verbosity in your voice response by displaying more in-depth information on the subject at hand, simplifying consumption of information for your customer. For example, a weather skill that provides the daily forecast could tell you the temperature and precipitation, while on a screen, it could display a breakdown richer weather data such as humidity, wind speed, and temperature hour by hour. All of the information is potentially important, but it would take far too long to even read out all of these stats, let alone an hour by hour breakdown! If the requesting device does not have a screen, more information such as humidity and wind speed can be read out to the customer. In both cases, you would want to allow them to ask for more details. Even if a user has a multimodal device, we can't assume they're always looking at the screen. Later in this course, we will cover the specifics of how to determine if the requesting Alexa device has a screen.

== Complementary visual aids
Multimodal devices provide an opportunity to add visual branding to your experience. You can add your brand logo and use your own color palette and layout styling to enhance the visual experience your customers are having and make it unique. For instance, if you have a business skill, you might want to show a chart of sales when talking about a sales summary with your skill logo at the top with colors to show gains and losses. High quality visuals improves your brand image for your skill in addition to making a more well-rounded customer experience. For instance, it is common in music skills to display visuals on screen devices. Showing album art is entirely tangential to the use of the skill to play music, but the customer gets a more rounded experience. If they look at the screen, they immediately know the service playing the music as well as the band and song. In these longer running skills, having a visual aid is particularly nice. Since branding is about having a constant reminder of the experience to the customer that is memorable without being obtrusive, this complements the audio experience with a visual that is relevant to the voice response, in addition to the experience you want to provide. 

In addition to helping your skill stand out, complementary visual aids are an important way to teach your customers in how to interact with your skill. Hints provide a place where you can nudge users to perform specific actions. Hint messages can be specific to the ongoing interaction that the user is currently having. For instance, in our weather skill above, you may want to provide a hint to the user to tell them that they can ask for a breakdown of the wind speed by hour if asking about the weather for the day. While this is possible to do with voice, it can be obtrusive if over done. By using the screen to convey tangential information, you are not impeding the customer or overloading them with information. 

== Rich media experiences
While you always need a voice experience, some skills are used primarily as a way to showcase media. Multimodal interfaces give you the ability to provide videos, images, and animations. Consider a skill which displays user photos, videos, and photo metadata from a user's hosted account. This would do very little on devices without screen, only having the functionality to read out the data about the content or play the audio of a video, but it would function fully and play the video or display images if used from a device with a screen. Multimodal devices give you the opportunity to do this which does not exist otherwise. 

== Alexa Presentation Language
The https://developer.amazon.com/docs/alexa-presentation-language/understand-apl.html[Alexa Presentation Language] (APL), is designed for rendering visuals across the ever increasing category of Alexa-enabled devices with screens, allowing you to add graphics, images, slideshows, video, and animations to create the visual experience you want in a way that is consistent across your skill. APL gives you reach with a design language that scales across the wide range of Alexa-enabled device types without individually targeting devices. In addition to screens, https://developer.amazon.com/docs/alexa-presentation-language/apl-reference-character-displays.html[APL has variations] to target non-screen multimodal devices such as the https://www.amazon.com/dp/B07N8RPRF7/[Echo dot with clock]. As the types of devices grows, APL grows with it. By understanding and learning to use Alexa Presentation Language, you will have the knowledge and tools needed to reach the most multimodal devices.

=== APL skill flow

image:APLSkillFlowDiagram.png[]

Alexa skills all follow a diagram similar to the one above. 

1. Customers talk to their Alexa-enabled device. 
2. This goes off to the Alexa service in the cloud to translate that voice into text, then text into an intention. 
3. The intent (with slots) goes to the correct backend which formulates an appropriate speakOutput with optional additional directives*. 
4. This response through Alexa services to perform text to speech from the skill's speakOutput and renders visuals on the device if the appropriate directive (RenderDocument) is sent. 

Not pictured are events which can trigger requests to the skill backend in response to user actions. These are similar to the handlers created for an intent or launch request. 

*Directives are simply messages which tell an Alexa device to perform an action

=== APL documents

An APL document is a holder for all of the definitions of UI elements and their visual hierarchy in a RenderDocument directive. APL documents also hold styles associated with those components, as well as points where you can bind data. The visual elements on the screen are made up of APL components, and layouts. https://developer.amazon.com/docs/alexa-presentation-language/apl-component.html[Components] are the most basic building blocks of APL and represent small, self-contained UI elements which display on the https://en.wikipedia.org/wiki/Viewport[viewport]. https://developer.amazon.com/docs/alexa-presentation-language/apl-layout.html[Layouts] are used like components, but are not primitive elements. Rather, they combine other layouts and primitive components to create a UI pattern. You can create your own layouts or import pre-defined layouts from other sources. 
Every APL document has a "mainTemplate". This simply represents the start state of the APL screen to be rendered. Here is an example blank APL document with all of its top-level properties: 

 {
     "type": "APL",
     "version": "1.1",
     "settings": {},
     "theme": "dark",
     "import": [],
     "resources": [],
     "styles": {},
     "onMount": [],
     "graphics": {},
     "commands": {},
     "layouts": {},
     "mainTemplate": {
         "parameters": [
             "payload"
         ],
         "items": []
     }
 }

Instead of defining all of the terms now, you will learn about the parts of an APL document in more detail as you progress through the course. Let's go to section 2 where we will use the APL authoring tool to create a simple APL document for our LaunchRequest for our Cake Walk skill.

link:module2.html[Next Module (2)]
