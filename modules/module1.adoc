
:imagesdir: ../modules/images
:toc:

= Why Build Multimodal Alexa Skills?

{blank}

A multimodal skill uses voice and other forms of communication, such as visual aids, to the skill experience. While Alexa is always voice first, adding visuals as a secondary mode can greatly enrich your customer's experience for Alexa-enabled devices with screens. There are tens of millions of multimodal Alexa devices used by consumers today. Currently, multimodal devices come in a variety of forms such as: the Echo Spot, Echo Show, FireTV, Fire tablets, and devices from other manufacturers with Alexa built-in. Adding visuals to your Alexa Skill can enhance the experience for your customers on these devices in the following ways:

== Increase the level of detail in your skill's response
Multimodal skills can provide more information through visual aids leading to a better customer experience. Voice is a fantastic way to interact with users because it's intuitive and efficient; however, it's not a great choice for presenting complex or large amounts of information at once. Instead, keep the voice response as succinct as possible while providing more detailed information in your visuals.

For example, a weather skill that provides the daily forecast could tell you the temperature and precipitation. During the voice response, it could display a breakdown of detailed weather data such as: hour by hour temperature, humidity, and wind speed. All of the information provided is potentially important, but it would take far too long for Alexa to read out all of these stats without customers getting frustrated and bored.

Later in this course, we'll cover the specifics of how to determine if a device has a screen. If the requesting device does not have a screen, more information such as humidity and wind speed can be read out to the customer after the most important basic information is given first. In both cases, you would want to allow them to ask for more details. Even if a user has a multimodal device, we can't assume they're always looking at the screen.

== Complementary visual aids
Multimodal devices provide an opportunity to add visual branding to your experience. You can add your own brand logo, color palette and styling to create a unique visual experience for your customers. For example, if you have a business skill, you might want to show a sales summary chart with your skill's logo at the top and set colors representing metrics such as gains and losses. High quality visuals improve your brand image by providing a well-rounded customer experience. For instance, it's common for music skills to display visuals such as album art on devices with a screen while playing music. If the customer looks at the screen, they immediately know which service is playing the music as well as the artist, album, and song. Music skills also run for long periods of time so having a visual aid can act as a constant, but unobtrusive reminder of the experience to the customer.

High quality visuals also provide an important way to teach your customers how to interact with your skill. One way is to use hints, a short (one sentence) message at the bottom of the screen that describes how you can perform specific actions. Hint messages can be specific to the ongoing interaction that the customer is currently having. For example, in a weather skill you may want to provide a hint that describes how the customer can ask for a breakdown of today's wind speed by hour. By using the screen to convey tangential information, you are not impeding the customer or overloading them with information by voice alone.

== Rich media experiences
While you always need a voice experience, some skills are used primarily as a way to showcase media such as videos, images, and animations. For instance, consider a skill which displays the customer's photos, videos, and media metadata from their hosted account. On a device without a screen, you'd only have it read out the metadata or play the audio portion of a video file. However, on a device with a screen you'd be able to display, play, and search for visual content easily.

== Alexa Presentation Language
The https://developer.amazon.com/docs/alexa-presentation-language/understand-apl.html[Alexa Presentation Language] (APL), is designed for rendering visuals across the ever increasing category of Alexa-enabled devices with screens. With APL, you can add graphics, images, slideshows, videos, and animations to create a consistent visual experience across your skill. APL automatically scales across the wide range of Alexa-enabled device types without you having to individually target devices. In addition to screens, https://developer.amazon.com/docs/alexa-presentation-language/apl-reference-character-displays.html[APL has variations] to target non-screen multimodal devices such as the https://www.amazon.com/dp/B07N8RPRF7/[Echo dot with clock]. By understanding and learning to use Alexa Presentation Language, you'll have the knowledge and tools needed to reach the most multimodal devices.

=== APL skill flow

image:APLSkillFlowDiagram.png[]

Alexa skills all follow a diagram similar to the one above.

A. A customer talks to their Alexa-enabled device.
B. The device records their speech and sends it to the Alexa service in the cloud. The recording is translated from voice into text, then text into an intent. An intent represents an action that fulfills the customer's request.
C. The intent (with slots) goes to the correct backend handler which formulates an appropriate speakOutput with optional additional directives. These directives tell the device to perform an action.
D. This response through Alexa services to perform text to speech from the skill's speakOutput and renders visuals on the device if the appropriate directive (RenderDocument) is sent.

Note: Some events can trigger requests to the skill's backend in response to user actions. These are similar to the handlers created for an intent or launch request.


=== APL documents

An APL document holds all of the definitions for UI elements and their visual hierarchy in a RenderDocument directive. It also holds styles associated with those components, as well as points where you can bind data. https://developer.amazon.com/docs/alexa-presentation-language/apl-component.html[Components] are the basic, primitive building blocks of APL that represent self-contained UI elements which are displayed in the https://en.wikipedia.org/wiki/Viewport[viewport]. https://developer.amazon.com/docs/alexa-presentation-language/apl-layout.html[Layouts] can combine primitive components and other layouts to create a UI pattern. You can create your own layouts or import pre-defined layouts from other sources.

Every APL document has a "mainTemplate". This simply represents the start state of the APL screen to be rendered. Here is an example blank APL document with all of its top-level properties:

 {
     "type": "APL",
     "version": "1.1",
     "settings": {},
     "theme": "dark",
     "import": [],
     "resources": [],
     "styles": {},
     "onMount": [],
     "graphics": {},
     "commands": {},
     "layouts": {},
     "mainTemplate": {
         "parameters": [
             "payload"
         ],
         "items": []
     }
 }

As you progress through the course, we'll cover the parts of an APL document in more detail. Continue to section 2 where you'll use the APL authoring tool to create a simple APL document for the Cake Walk skill's LaunchRequest.

link:module2.html[Next Module (2)]
