
:link-cakewalk: https://developer.amazon.com/en-US/alexa/alexa-skills-kit/courses/cake-walk[Cake Walk Course]
:link-quick-setup: link:quickStart.html[quick setup instructions] 
:link-S3-assets: https://github.com/alexa/skill-sample-nodejs-first-apl-skill/tree/master/modules/assets[from this page.]
:authoringToolLink: https://developer.amazon.com/alexa/console/ask/displays
:sectnums:
:toc:

:imagesdir: ../modules/images

= Modify Your Backend to Add LaunchRequest Visuals

{blank}

In this section, you will learn how to modify your backend to safely add APL visuals to your response. At the end, you will have enhanced your Cake Walk Launch request with the APL visuals you built in the last section. In addition, you will learn how to use S3 in the Alexa Hosted environment to host your own assets and use this to add a background image to your skill. 

Cake Walk code will be used in this section. If you have not done the {link-cakewalk}, and you are a new Alexa developer, check out the intro course, now, then come back to this module. If you are already a skill builder and experienced with the basics of a nodeJS skill, then feel free to skip the course and see the {link-quick-setup} to get the code and overview of the Cake Walk Skill. 

== Adding visuals to your skill

For an Alexa device to safely render your APL document, you have to respond with a render directive only when the device supports APL. This directive takes the form of `Alexa.Presentation.APL.RenderDocument`. You can add a directive using the nodeJS SDK easily by calling `handlerInput.responseBuilder.addDirective({...});`

Even though you can add a render directive to every response, not all devices can react to this. In order to safely respond with the `Alexa.Presentation.APL.RenderDocument`, you must first make sure the calling device sends the proper request object. This information is contained in the supportedInterfaces object found like `context.System.device.supportedInterfaces`. For example, your sample request might look like:

 {
	...
	"context": {
		"System": {
			...
			"device": {
				"deviceId": "amzn1.ask.device.AGXTGBOJAX3WK4YUKWCQ2CILA6S35XSANHUIPFZ6IEOYQG234XDJFP5WVLREJDWHV6PYBEHRNDUYCZMIM52ED3XO3VY7LBBEFT7JZQZW4SKD4Q3BUYQPLAEPGGF5VWAVLZXT4I4LXX5DGLRKEWGI5GLVCRZPUZK5I2IX5LMK5V2FXKJ4A2NLC",
				"supportedInterfaces": {
					"Alexa.Presentation.APL": {
						"runtime": {
							"maxVersion": "1.0"
						}
					}
				}
			}
			...
		}
 	}
 }

If you are using the SDK, checking for the supported interfaces is simple.
A. In your `LaunchRequestHandler.handle()` before the response is created, add:
+
 if (Alexa.getSupportedInterfaces(handlerInput.requestEnvelope)['Alexa.Presentation.APL']) {
     // Create Render Directive.
 }
+
This if statement is going to check if the APL interface is sent in the request envelope. Only then, do we want to add the response. Now, let's add the response. 
B. Underneath the `// Create Render Directive` comment, add:
+
 handlerInput.responseBuilder.addDirective({
     type: 'Alexa.Presentation.APL.RenderDocument',
     version: '1.1',
     document: launchDocument,
     datasources: {
        text: {
            type: 'object',
            start: "Welcome",
            middle: "to",
            end: "Cake Walk!"
        }
    }
 });
+
Now, let's import our launchDocument. We will be naming our document launchDocument.json since it represents the APL Document for the generic Launch Requests.
+
image:createLaunchScreenJSON.gif[]
+
C. On the left, select the "lambda" folder. Right-click and select "Create Folder". Call this folder "documents". Add a new file inside the new folder called `launchDocument.json`.
D. Inside this new file, add the JSON from the authoring tool we created in module 2 (the full JSON document is at the end of module 2).
E. Then, import this at the top of your `index.js` like so:
+
 const launchDocument = require('./documents/launchDocument.json')
+
The launchDocument variable in your javascript code represents the contents of your json file. 
F. Now, deploy the code. But wait, we cannot test it, yet.

== Using the developer console simulator to test

Before we can test our document changes, we have to enable APL in the Build tab. 

A. Click on the build Tab. You will need to click on Interfaces on the left side.
+
image::interfacesClick.png[]
+
B. Scroll down and toggle the Alexa Presentation Language and you will see the block expand. Select all of the boxes. 
+
Hub Round, Hub Landscape, and TV Fullscreen are selected by default and must be supported. Each of these are screens you will be officially supporting. They will not scale your content, so whatever assets and APL layout you send in the response will be shown as is. This also means whatever viewport profiles are not supported will scale your content from the closest sized viewport you do support. Select them all because we will be verifying and testing each of these profiles. By selecting these, you are saying you tested your layout on each of these profiles.
+
image::toggleAPL.png[]
+
C. After you toggle APL on and select all of the viewport profiles, you must build the interaction model. Note: this also means changes to the supported profiles need a new skill certification round in order to go live.
+
D. After building, you are ready to test. Go to the test tab and make sure "Device Display" is checked. Now if you scroll down past the request/response json, you can select the device you want to try. 
+
TIP: Selecting a device type will reset your skill session.
+
E. Test your screen on a medium hub by triggering the launch handler for instance by typing, `open cake walk`. 
+
TIP: Remember to delete your user data if you have any! Cake walk has two different LaunchRequest handlers - One which is invoked when there is no context while the other is invoked when your birthday is known. Since we modified the no context launch handler, you must delete your user data from S3 to make sure there is actually no context. You can do this by accessing the S3 provision ("Media storage: S3..."), clicking "amzn1-ask-skill-...", and then deleting the document that starts with "amzn1.ask.account...".  
+
F. Repeat this test on all the other screen types. 


== Uploading assets to S3
Before we can make use of image assets, we will need to host these images. Ideally, you will use a https://en.wikipedia.org/wiki/Content_delivery_network[CDN] (such as https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/[Amazon Cloudfront in conjunction with S3]) to serve your assets to locations closer to your users, but for this exercise, we will use S3 since this is provided by Alexa hosted. 

A. To upload the S3 Assets, you can access your S3 provision under the code tab at the very bottom on the left side. Go here now.
+ 
image::S3Access.png[]
+
B. In your S3 provision, you will open up into the Media page. 
C. In here, upload all of the assets {link-S3-assets}. We will use these over the course of the module. After you upload, you will see all of the assets on the page in S3 like so.
+
image::S3Provision.png[]

Now that your assets are uploaded, we can update our Launch Request with images.

== Updating our launch request handler

Now that we have our no context launch request working, it is time to add an image rather than using the default background. 

A. To do so, first open up the {authoringToolLink}[authoring tool] and paste in the launchDocument.json information. 
B. Then in the data section, use:
+
 {
    "text": {
        "start": "Welcome",
        "middle": "to",
        "end": "Cake Walk!"
    },
    "assets": {
        "cake":"https://github.com/alexa/skill-sample-nodejs-first-apl-skill/blob/master/modules/assets/alexaCake_960x960.png?raw=true",
        "backgroundURL": "https://github.com/alexa/skill-sample-nodejs-first-apl-skill/blob/master/modules/assets/lights_1920x1080.png?raw=true"
    }
 }
+
You will notice a new field in our data, backgroundURL, under the assets object. This represents where the device will fetch a background image from. We will use the Github repo for hosting it for now while we develop the screen, but our actual code will use the S3 presigned link util function. Now, we need to add our background component. 
C. Go back to the APL tab in the tool.
D. We are going to add the https://developer.amazon.com/docs/alexa-presentation-language/apl-alexa-background-layout.html[AlexaBackground responsive component]. To use this, you need the alexa-layouts package which we already have! Using the AlexaBackground is easy, just add the following to the top of your containers in the items array:
+
 {
    "type": "AlexaBackground",
    "backgroundImageSource": "${payload.assets.backgroundURL}"
 },
+
You should see the background light up...er...see the lights in the background. 
E. Apply the same changes to the `@hubRoundSmall` variation and ensure it works in the authoring tool. 

IMPORTANT: Make sure the AlexaBackground responsive component is above the other components, otherwise it will occlude them!

You may notice we are using a single 1920x1080 png for each of the devices and it scales pretty well. We want to use the highest possible resolution to consider FireTV devices. Scaling down produces a better quality image. The tradeoff is that smaller resolution devices which do not support this quality level will download unnecessary data. The best course of action would be to provide two or more different image resolutions for different devices classes. We will see how to do this in the next section. 

== Applying our images

A. Now that we have our document ready, replace the launchDocument.json contents in your Code tab with the JSON from the authoring tool. 
B. Go back to the index.js. Since we are adding in links to our private S3 instance, we will need to import the util module. At the top of this file, add in another import:
+
 const util = require('./util');
+
C. We must add our new datasources to our code in the index.js. Since our images are in the non-public S3 bucket, we are going to be using the util function to get a short lived public URL to the asset. The S3 object keys in this case are going to be of the form, `'Media/imageName.png'`. Lets add our images inside the APL render block's data. Our datasources block will now look like:
+
 datasources: {
    text: {
        type: 'object',
        start: "Welcome",
        middle: "to",
        end: "Cake Walk!"
    },
    assets: {
        cake: util.getS3PreSignedUrl('Media/alexaCake_960x960.png'),
        backgroundURL: util.getS3PreSignedUrl('Media/lights_1920x1080.png')
    }
 }
+
D. Deploy and test your new document on each of the screen sizes.
E. But wait, we aren't fully done yet. We are going to add the optimization we mentioned in the last section. This requires another asset. We have already uploaded the lights_1280x800.png. We need to change out images to conditionally pull the right asset. Replace the value for our backgroundURL with:
+
 util.getS3PreSignedUrl(backgroundKey)
+
F. To conditionally set the proper `backgroundKey`, we need to use the Ask-sdk-core which we already have imported as `Alexa`. To get the viewport profile, inside your APL conditional, add:
+
 const viewportProfile = Alexa.getViewportProfile(handlerInput.requestEnvelope);
+
This will return a https://developer.amazon.com/docs/alexa-presentation-language/apl-viewport-characteristics.html#access-the-viewport-information-in-your-code[ViewportProfile] such as `HUB-LANDSCAPE-LARGE`. What we want to do is to check if the requesting device a TV or a Hub device. If it is a TV (XLARGE), we will use the large asset, but if it is a hub, we will deliver the smaller asset. Ideally, you would have an asset for each type of profile, but since we are using a responsive component, this is really only saving some bandwidth on lower resolution devices. 
G. We can implement this logic with the statement added below the viewportProfile statement:
+
 const backgroundKey = viewportProfile === 'TV-LANDSCAPE-XLARGE' ? "Media/lights_1920x1080.png" : "Media/lights_1280x800.png";
+
H. Test this out making sure to use the TV and the hub devices in the test console. 

Since our frontend is scaling properly by using the responsive components, we are done! Let's head to the next module and learn some more advanced APL document concepts.

https://github.com/alexa/skill-sample-nodejs-first-apl-skill/tree/master/modules/code/module3[Complete code in Github]

link:module2.html[Previous Module (2)]
link:module4.html[Next Module (4)]
